{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from tqdm import tnrange\n",
    "#use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 65628 entries, 0 to 65627\n",
      "Data columns (total 25 columns):\n",
      " #   Column                        Non-Null Count  Dtype\n",
      "---  ------                        --------------  -----\n",
      " 0   Unnamed: 0                    65628 non-null  int64\n",
      " 1   countryName                   65628 non-null  int64\n",
      " 2   eprtrSectorName               65628 non-null  int64\n",
      " 3   EPRTRAnnexIMainActivityLabel  65628 non-null  int64\n",
      " 4   FacilityInspireID             65628 non-null  int64\n",
      " 5   facilityName                  65628 non-null  int64\n",
      " 6   City                          65628 non-null  int64\n",
      " 7   targetRelease                 65628 non-null  int64\n",
      " 8   pollutant                     65628 non-null  int64\n",
      " 9   reportingYear                 65628 non-null  int64\n",
      " 10  MONTH                         65628 non-null  int64\n",
      " 11  DAY                           65628 non-null  int64\n",
      " 12  CONTINENT                     65628 non-null  int64\n",
      " 13  max_wind_speed                65628 non-null  int64\n",
      " 14  avg_wind_speed                65628 non-null  int64\n",
      " 15  min_wind_speed                65628 non-null  int64\n",
      " 16  max_temp                      65628 non-null  int64\n",
      " 17  avg_temp                      65628 non-null  int64\n",
      " 18  min_temp                      65628 non-null  int64\n",
      " 19  DAY WITH FOGS                 65628 non-null  int64\n",
      " 20  REPORTER NAME                 65628 non-null  int64\n",
      " 21  CITY ID                       65628 non-null  int64\n",
      " 22  Unnamed: 22                   65628 non-null  int64\n",
      " 23  EPRTRAnnexIMainActivityCode   65628 non-null  int64\n",
      " 24  EPRTRSectorCode               65628 non-null  int64\n",
      "dtypes: int64(25)\n",
      "memory usage: 12.5 MB\n"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv(\"train_total.csv\",index_col=False)\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65628, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_3=['countryName','EPRTRAnnexIMainActivityLabel','FacilityInspireID','City',\n",
    "       'MONTH','max_wind_speed', 'avg_wind_speed', \n",
    "       'min_wind_speed','max_temp','avg_temp', 'min_temp','DAY WITH FOGS', 'REPORTER NAME']\n",
    "x=train[pca_3]#.values\n",
    "y=train['pollutant']#.values#.reshape((65628,1))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0,test_size=0.3)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, random_state=0,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 4594\n"
     ]
    }
   ],
   "source": [
    "f=10 # no. of batches #15\n",
    "\n",
    "train_batch = np.array_split(x_train, f) \n",
    "label_batch = np.array_split(y_train, f) \n",
    "\n",
    "val_batch = np.array_split(x_val, f)\n",
    "val_label_batch = np.array_split(y_val, f)\n",
    "\n",
    "test_batch = np.array_split(x_test, f)\n",
    "test_label_batch = np.array_split(y_test, f)\n",
    "\n",
    "\n",
    "for i in range(len(train_batch)):\n",
    "    train_batch[i] = torch.from_numpy(train_batch[i].values).float()\n",
    "for i in range(len(label_batch)):\n",
    "    label_batch[i] = torch.from_numpy(label_batch[i].values).float().view(-1, 1)\n",
    "\n",
    "for i in range(len(val_batch)):\n",
    "    val_batch[i] = torch.from_numpy(val_batch[i].values).float()\n",
    "for i in range(len(val_label_batch)):\n",
    "    val_label_batch[i] = torch.from_numpy(val_label_batch[i].values).float().view(-1, 1)\n",
    "    \n",
    "for i in range(len(test_batch)):\n",
    "    test_batch[i] = torch.from_numpy(test_batch[i].values).float()\n",
    "for i in range(len(test_label_batch)):\n",
    "    test_label_batch[i] = torch.from_numpy(test_label_batch[i].values).float().view(-1, 1)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "print(\"Batch size:\", len(train_batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):#87%\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(13, 26)\n",
    "        self.fc2 = nn.Linear(26, 52)\n",
    "        self.fc3 = nn.Linear(52, 104)\n",
    "        self.fc4 = nn.Linear(104, 208)\n",
    "        self.fc5 = nn.Linear(208, 104)\n",
    "        self.fc6 = nn.Linear(104, 26)\n",
    "        self.fc7 = nn.Linear(26, 13)\n",
    "        self.fc8 = nn.Linear(13, 1)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = F.relu(self.fc8(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "model = Regressor()\n",
    "train_losses, val_losses = [], []\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adamax(model.parameters(), lr=0.01) # 0.015 87\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.01, patience=15) \n",
    "total_epochs=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ivanl\\AppData\\Local\\Temp/ipykernel_4996/3967188832.py:4: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for e in tnrange(epochs):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceca4eb8cd384ae6bc4973a5915e017b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 train_loss : 200481.75813138485 Val loss:  220.73100996017456\n",
      "Epoch : 1 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 2 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 3 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 4 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 5 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 6 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 7 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 8 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 9 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 10 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 11 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 12 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 13 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 14 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 15 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 16 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 17 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 18 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 19 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 20 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 21 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 22 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 23 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 24 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 25 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 26 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 27 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 28 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 29 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 30 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 31 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 32 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 33 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 34 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 35 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 36 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 37 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 38 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 39 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 40 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 41 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 42 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 43 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 44 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 45 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 46 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 47 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 48 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 49 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 50 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 51 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 52 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 53 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 54 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 55 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 56 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 57 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 58 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 59 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 60 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 61 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 62 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 63 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 64 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 65 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 66 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 67 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 68 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 69 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 70 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 71 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 72 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 73 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 74 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 75 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 76 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 77 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 78 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 79 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 80 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 81 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 82 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 83 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 84 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 85 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 86 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 87 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 88 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 89 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 90 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 91 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 92 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 93 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 94 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 95 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 96 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 97 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 98 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 99 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 100 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 101 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 102 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 103 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 104 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 105 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 106 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 107 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 108 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 109 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 110 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 111 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 112 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 113 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 114 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 115 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 116 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 117 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 118 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 119 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 120 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 121 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 122 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 123 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 124 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 125 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 126 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 127 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 128 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 129 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 130 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 131 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 132 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 133 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 134 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 135 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 136 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 137 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 138 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 139 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 140 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 141 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 142 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 143 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 144 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 145 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 146 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 147 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 148 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 149 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 150 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 151 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 152 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 153 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 154 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 155 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 156 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 157 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 158 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 159 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 160 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 161 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 162 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 163 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 164 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 165 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 166 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 167 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 168 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 169 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 170 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 171 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 172 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 173 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 174 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 175 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 176 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 177 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 178 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 179 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 180 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 181 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 182 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 183 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 184 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 185 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 186 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 187 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 188 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 189 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 190 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 191 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 192 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 193 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 194 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 195 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 196 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 197 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 198 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 199 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 200 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 201 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 202 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 203 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 204 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 205 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 206 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 207 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 208 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 209 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 210 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 211 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 212 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 213 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 214 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 215 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 216 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 217 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 218 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 219 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 220 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 221 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 222 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 223 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 224 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 225 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 226 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 227 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 228 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 229 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 230 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 231 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 232 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 233 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 234 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 235 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 236 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 237 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 238 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 239 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 240 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 241 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 242 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 243 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 244 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 245 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 246 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 247 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 248 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 249 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 250 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 251 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 252 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 253 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 254 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 255 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 256 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 257 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 258 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 259 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 260 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 261 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 262 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 263 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 264 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 265 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 266 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 267 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 268 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 269 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 270 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 271 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 272 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 273 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 274 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 275 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 276 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 277 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 278 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 279 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 280 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 281 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 282 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 283 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 284 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 285 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 286 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 287 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 288 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 289 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 290 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 291 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 292 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 293 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 294 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 295 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 296 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 297 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 298 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 299 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 300 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 301 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 302 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 303 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 304 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 305 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 306 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 307 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 308 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 309 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 310 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 311 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 312 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 313 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 314 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 315 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 316 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 317 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 318 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 319 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 320 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 321 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 322 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 323 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 324 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 325 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 326 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 327 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 328 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 329 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 330 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 331 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 332 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 333 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 334 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 335 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 336 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 337 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 338 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 339 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 340 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 341 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 342 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 343 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 344 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 345 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 346 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 347 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 348 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 349 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 350 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 351 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 352 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 353 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 354 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 355 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 356 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 357 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 358 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 359 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 360 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 361 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 362 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 363 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 364 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 365 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 366 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 367 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 368 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 369 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 370 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 371 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 372 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 373 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 374 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 375 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 376 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 377 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 378 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 379 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 380 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 381 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 382 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 383 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 384 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 385 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 386 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 387 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 388 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 389 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 390 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 391 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 392 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 393 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 394 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 395 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 396 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 397 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 398 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n",
      "Epoch : 399 train_loss : 1.847102439403534 Val loss:  1.8093427896499634\n"
     ]
    }
   ],
   "source": [
    "epochs = 400\n",
    "total_epochs+=epochs\n",
    "\n",
    "for e in tnrange(epochs):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    val_loss_1=0\n",
    "    val_loss_sum = 0\n",
    "    \n",
    "    for i in range(len(train_batch)):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_batch[i])\n",
    "        loss = criterion(output, label_batch[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for j in range(len(val_batch)):\n",
    "                \n",
    "                val_output = model(val_batch[j])\n",
    "                val_loss =  criterion(val_output, val_label_batch[j])\n",
    "                val_loss_1+=val_loss.item()\n",
    "        val_loss_sum=val_loss_1/len(val_batch)\n",
    "        \n",
    "        \n",
    "    print(\"Epoch :\", e, \"train_loss :\", train_loss/len(train_batch), \"Val loss: \", val_loss_sum/len(val_batch))    \n",
    "    val_losses.append(val_loss_sum/len(val_batch))    \n",
    "    train_losses.append(train_loss/len(train_batch))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1fc1bdd40d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "frm=10 # does not \n",
    "plt.figure(figsize=(25,15))\n",
    "plt.plot(train_losses[frm:], label='Training loss')\n",
    "plt.plot(val_losses[frm:], label='Validation loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "#ax = plt.gca()\n",
    "#ax.set_ylim([0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      1.00      0.53      3520\n",
      "           1       0.00      0.00      0.00      2451\n",
      "           2       0.00      0.00      0.00      3873\n",
      "\n",
      "    accuracy                           0.36      9844\n",
      "   macro avg       0.12      0.33      0.18      9844\n",
      "weighted avg       0.13      0.36      0.19      9844\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Miniconda3\\envs\\tfg\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\Miniconda3\\envs\\tfg\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\Miniconda3\\envs\\tfg\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "correct=0\n",
    "i=0\n",
    "y_pred=[]\n",
    "with torch.no_grad():\n",
    "    for batch in test_batch :\n",
    "        for j in range(len(batch)):\n",
    "            x = model(batch[j])\n",
    "            #print(round(x.item()))\n",
    "            y_pred.append(round(x.item()))\n",
    "#print(y_pred)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3132f7a729275460905ce50538cd1987892ffc1102ba7dba42e211caddde65d4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('tfg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
